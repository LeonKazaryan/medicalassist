import json
import re
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
from config import MODEL_PATH, DB_PATH, COLLECTION_NAME

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
model = SentenceTransformer(MODEL_PATH)
client = QdrantClient(path=DB_PATH)

def clean_medical_text(text):
    if not text: return ""
    text = re.sub(r'\[\d+[\d\s,\-]*\]', '', text)
    text = re.sub(r'\(\s*–£–î\s*-\s*[A-Z–ê-–Ø]\s*\)', '', text)
    text = re.sub(r'\b\d+(\.\d+)+\b', '', text)
    text = re.sub(r'[ÔÇ∑‚Ä¢\t\-_‚Äì‚Äî]', ' ', text)
    text = text.replace('\n', ' ').replace('\r', ' ')
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def chunk_text(text, chunk_size=1000, overlap=200):
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start += (chunk_size - overlap)
    return chunks

def clean_title(title):
    t = title.replace("–î–ò–ê–ì–ù–û–°–¢–ò–ö–ò –ò –õ–ï–ß–ï–ù–ò–Ø", "").replace("–ö–õ–ò–ù–ò–ß–ï–°–ö–ò–ô –ü–†–û–¢–û–ö–û–õ", "")
    t = re.sub(r'\s+[IVX1-9]$', '', t) 
    return t.strip()

def extract_all_codes(item):
    # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –ø–µ—Ä–≤—ã—Ö 2000 —Å–∏–º–≤–æ–ª–æ–≤ (—Ç–∞–º –æ–±—ã—á–Ω–æ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–¥—ã)
    intro_text = " ".join([s.get("content", "") for s in item.get("sections", [])])[:2000]
    intro_text += " " + " ".join(item.get("icd_codes", []))
    
    intro_text = intro_text.upper().translate(str.maketrans("–û–ê–í–°–ö–ú–ï", "OABCKME"))
    
    # –†–µ–≥—É–ª—è—Ä–∫–∞ –¥–ª—è –∫–æ–¥–æ–≤
    codes = re.findall(r'[A-Z]\d{2}(?:\.\d{1,2})?', intro_text)
    
    # –£–±–∏—Ä–∞–µ–º "–≤–∏—Ç–∞–º–∏–Ω–Ω—ã–µ" –∏ —à—É–º–æ–≤—ã–µ –∫–æ–¥—ã, –µ—Å–ª–∏ –µ—Å—Ç—å –¥—Ä—É–≥–∏–µ
    clean_codes = [c for c in codes if not c.startswith(('B12', 'E55', 'D64'))]
    if not clean_codes: clean_codes = codes
    
    return list(dict.fromkeys(clean_codes))

def ingest_from_json(file_path):
    print(f"üîÑ –ü–µ—Ä–µ—Å–æ–∑–¥–∞—é –∫–æ–ª–ª–µ–∫—Ü–∏—é {COLLECTION_NAME}...")
    client.recreate_collection(
        collection_name=COLLECTION_NAME,
        vectors_config=VectorParams(size=384, distance=Distance.COSINE),
    )

    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f) 

    points = []
    idx = 0

    for item in tqdm(data, desc="–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è"):
        protocol_id = item.get("protocol_id")
        raw_title = item.get("true_title", "Unknown")
        title = clean_title(raw_title)
        
        # –í–´–¢–ê–°–ö–ò–í–ê–ï–ú –í–°–ï –ö–û–î–´ –î–õ–Ø –í–°–ï–ì–û –ü–†–û–¢–û–ö–û–õ–ê –û–î–ò–ù –†–ê–ó
        all_protocol_codes = extract_all_codes(item)
        icd_str = ", ".join(all_protocol_codes)
        
        for section in item.get("sections", []):
            sec_type = section.get("type", "unknown")
            # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–∂–Ω—ã–µ —Å–µ–∫—Ü–∏–∏ –¥–ª—è Accuracy
            if sec_type not in ["complaints", "criteria", "definition"]:
                continue

            content = clean_medical_text(section.get("content", ""))
            if len(content) < 30: continue
            
            chunks = chunk_text(content)
            for chunk in chunks:
                # –í–ï–ö–¢–û–† –¢–ï–ü–ï–†–¨ –í–ö–õ–Æ–ß–ê–ï–¢ –í–°–ï –ö–û–î–´!
                text_to_vector = f"passage: –ü–†–û–¢–û–ö–û–õ: {title}. –ö–û–î–´ –ú–ö–ë: {icd_str}. –¢–ï–ö–°–¢: {chunk}"
                
                vector = model.encode(text_to_vector).tolist()
                
                points.append(PointStruct(
                    id=idx,
                    vector=vector,
                    payload={
                        "protocol_id": protocol_id,
                        "title": title,
                        "icd_codes": all_protocol_codes, # –¢–£–¢ –¢–ï–ü–ï–†–¨ –°–ü–ò–°–û–ö –ö–û–î–û–í
                        "section": sec_type,
                        "content": chunk
                    }
                ))
                idx += 1
                
                if len(points) >= 100:
                    client.upsert(COLLECTION_NAME, points)
                    points = []
                
    if points:
        client.upsert(COLLECTION_NAME, points)
    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –°–æ–∑–¥–∞–Ω–æ {idx} –≤–µ–∫—Ç–æ—Ä–æ–≤.")

if __name__ == "__main__":
    ingest_from_json("processed_protocols.json")